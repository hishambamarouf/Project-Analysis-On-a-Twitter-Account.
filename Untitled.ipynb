{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRANGLING AND ANALYZING THE \"WeRateDogs\" TWITTER ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Wrangling a dataset consists of three parts:\n",
    "       - Gathering \n",
    "       - Assessing \n",
    "       - Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering:\n",
    "\n",
    "    For this project, i gathered three datasets. The first one was the (twitter_archive_enhanced.csv) which was provided Udacity, i manually downloaded it and uploaded it to my Jupyter notebook. The second one was the (image_predictions.tsv) which is hosted on the udacity web server, i used the Requests library, alongside the URL to the dataset to request it and save it to my notebook, the third one is the (tweet_json.csv), to obtain it, i created a twitter account and signed up for a developer account, then i fed the consumer and secret key to the Tweepy API and wrote each tweet into tweet_json.txt then extracted each tweet's id, retweet count and favourite count into a pandas dataframe and saved it as tweet_json.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing \n",
    "\n",
    "    i assessed the data in each dataset programmatically at first using pandas methods like .head() .info() .value_counts() .duplicated().sum(). Then to take a closer look i downloaded the data and opened it on excell. i found a few mistakes mostly in the text column which i documented in my notebook. and i found the following issues:\n",
    "    \n",
    "    # Issues\n",
    "\n",
    "### Quality\n",
    "    - Change Timestamp to datetime64 instead of String\n",
    "    - Fix numerator and denominator's wrong values\n",
    "    - Drop useless columns\n",
    "    - Change missing name values from \"NaN\" to None\n",
    "    - Change missing floofer, pupper, doggo and puppo from \"NaN\" to None\n",
    "    - Reduce dogs with multiple stage_name values to 1 stage name\n",
    "    - Drop invalid names\n",
    "    - Drop duplicated images\n",
    "    - Change source values to ('Iphone' , 'Vine' , 'Twitter_Web_Client' , 'TweetDeck')\n",
    "\n",
    "### Tidiness\n",
    "    - Combine the doggo, floofer, pupper and puppo columns into \"dog_stage\" column with type String\n",
    "    - Merge all the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "    As i didnt find any issues with the Tweet_json dataset, i created a copy of the archive and prediction dataframes to not miss with the original data, i started with the quality issues then i merged the three dataframes into one and saved it to a seperate csv called 'twitter_archive_master.csv', after that i did analysis on the following questions: \n",
    "    - Which is the most common source used.\n",
    "    - What is the most favourited dog breed.\n",
    "    - What are the most common false predictions in the dog prediciton algorithim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
